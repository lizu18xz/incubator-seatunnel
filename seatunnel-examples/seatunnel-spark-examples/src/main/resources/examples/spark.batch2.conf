
env {
  # You can set spark configuration here
  # see available properties defined by spark: https://spark.apache.org/docs/latest/configuration.html#available-properties
  spark.app.name = "SeaTunnel"
  spark.executor.instances = 2
  spark.executor.cores = 1
  spark.executor.memory = "1g"
  spark.master = local
}

## 从mysql读取数据-->做两种转换-->过滤/生成列-->分别输出到mysql 和 hdfs
## 从mysql读取数据-->做两种转换-->过滤-->cache-->分别输出到mysql 和 hdfs  验证

source {
  jdbc {
      driver = "com.mysql.jdbc.Driver"
      url = "jdbc:mysql://localhost:3306/test?verifyServerCertificate=false&useSSL=false"
      table = "tagdata"
      user = "root"
      password = "bYR-KFa-AEJ-Y9U2018"
      result_table_name = "tag_data_temp"
  }
}

transform {

   cache {
          storage_level = "MEMORY_ONLY"
          result_table_name="temp_cache"
   }

}

sink {

  file {
      source_table_name = "temp_cache"
      path = "hdfs://0.0.0.0:9000/sparkx/data/tmp/tagdata"
      serializer = "csv"
      save_mode="overwrite"
      path_time_format="yyyyMMddHHmmss"
  }

  file {
      source_table_name = "temp_cache"
      path = "hdfs://0.0.0.0:9000/sparkx/data/tmp/tagdata2"
      serializer = "csv"
      save_mode="overwrite"
      path_time_format="yyyyMMddHHmmss"
  }

}
